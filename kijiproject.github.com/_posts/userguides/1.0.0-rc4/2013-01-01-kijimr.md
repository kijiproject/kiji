---
title: Kiji MapReduce
layout: post
categories: [userguides, mapreduce, 1.0.0-rc4]
tags: [mapreduce-ug]
version: 1.0.0-rc4
order : 10
description: Kiji MapReduce user guide.
---

# Overview

Kiji MapReduce is the second major component in the Kiji ecosystem for Apache Hadoop and HBase after KijiSchema.

Kiji MapReduce includes APIs to build MapReduce jobs that read and write data stored in Kiji tables, bringing MapReduce-based analytic techniques to a broad base of KijiSchema users, to support applications such as machine learning and analysis.

KijiMR is organized around three core MapReduce job types: _Bulk Importers_, _Producers_ and _Gatherers_. These may read from external data sources through _KeyValueStores_.

 * _Bulk Importers_ make it easy to efficiently load data into Kiji tables from a variety of formats, such as JSON or CSV files in HDFS clusters.
 * _Producers_ are entity-centric operations that use a entity's existing data to generate new information and store it back in the entity's row. One typical use-case for producers is to generate new recommendations for a user based on the user's history.
 * _Gatherers_ provide flexible MapReduce computations that scan over Kiji table rows and output key-value pairs. By using different outputs and reducers, Gatherers can export data to external data stores such as files in HDFS clusters in a variety of formats (such as text or avro) or even into HFiles to efficiently load data into other Kiji Tables.

Finally, Kiji MapReduce allows any of these jobs to combine their data with external _KeyValueStores_ as a way to join data sets store in HDFS, in Kiji, etc.

Unlike Kiji Schema, where the classes most relevant to application developers were usually concrete, these core job types exist in Kiji MapReduce as abstract classes (such as `KijiProducer`). It is typically up to the application developer to subclass the appropriate class in their application and implement their application's analysis logic in a few methods (such as `KijiProducer`'s `produce()` and `getDataRequest()`). They can then point the job at the appropriate Kiji table using either the `kiji` command-line tools or programmatically using one of the framework's JobBuilders (such as `KijiProduceJobBuilder`) that make launching these jobs easy.

To provide a starting point and some useful precanned solutions, the complementary Kiji MapReduce Library provides a growing repository of sample implementations that can be used directly or as example code. Both Kiji MapReduce and the Kiji MapReduce Library are included in the Kiji Bento Box distribution.

In the sections of this guide that follow, the core job types will be explained in greater detail, including motivation, example code snippets, and where appropriate, a description of the Kiji MapReduce Library's reference implementations. This guide also contains an in-depth description of how to use Kiji MapReduce's _KeyValueStores_ to expose data stored in different places and formats (such as HDFS or KijiTables) through a consistent interface to your MapReduce jobs. For power users, we've included instructions on how to use the `KijiMapReduceJobBuilder` class and `kiji mapreduce` command to build and launch arbitrary MapReduce jobs using Kiji mappers and reducers to perform flexible data transformation.

Finally this guide contains a description of the command-line tools included with Kiji MapReduce and facilities that make it easier to test Kiji MapReduce application code.

# Kiji MapReduce framework

## Bulk Importing

### Motivation

Before we can analyze any data in a Kiji table, we have to get that data into the Kiji table.

Using Kiji Schema alone, one can load data into a Kiji table from a single machine using a simple program with a while loop. For very small jobs, the speed of one machine may be fast enough, but for bigger jobs, a distributed approach is needed. With a little elaboration, one could distribute the simple program's work in the form of a MapReduce job whose mappers write to the Kiji table in parallel. However, writing directly to Kiji Schema's underlying HBase from a MapReduce job can introduce heavy load to a cluster making things sluggish or even unstable.

To more efficiently import data into Kiji tables, Kiji MapReduce includes _BulkImporters_. A bulk importer is a MapReduce job that processes its input into files that can be bulk loaded directly into Kiji. The format of the input and how it translates into Kiji table entity IDs and columns are details particular to each concrete subclass of `KijiBulkImporter`.

Kiji MapReduce Lib contains a set of sample parsers and bulk importers that enable users to import data for many common data formats (including CSV, JSON, and the Apache Common Log Format) into a Kiji table using an Avro description of how the data maps into the table's layout. See the _Provided Library Classes_ section below for more information.

### Classes Overview

Kiji bulk importers rely on two classes:
* _KijiBulkImporter_ - the base class for concrete bulk importers.  Subclasses must implement the `produce()` method for converting the raw data into Kiji puts, as well as the `setup()` and `teardown()` methods if they wish to perform any special configuration at task startup or shutdown respectively.
* _KijiBulkImportJobBuilder_ - a MapReduce job builder that allows the creation of bulk import jobs using concrete bulk importers.

Bulk-importers inherit from `KijiBulkImporter` and must implement their bulk-importing logic in the `produce()` method.
Optionally, Bulk-importers may use the `setup()` and `cleanup()` methods to initialize and finalize resources that can be shared across input records.
These methods will be called once by each task, `setup()` before processing input records and `cleanup()` after the task is done processing input.

### Using the API

If one of the precanned bulk importers listed above is insufficient for the data that is to be imported,
the KijiBulkImporter or the DescribedTextBulkImporter class can be extended to support parsing and importing the desired data.
The produce method needs to be implemented to handle the extraction of data from the import text.

Once you have the requisite bulk-importer, importing the data can simply follow these steps:

*   Define any necessary configuration for the bulk importer.
    For example, the bulk importers in Kiji MapReduce Library that use the DescribedInputTextBulkImporter all require a table import descriptor to map between source data and destination data.

*   Once the proper configuration file has been written, the data can be bulk imported into Kiji via commands like:

### Example

{% highlight java %}
/**
 * Example of a Bulk-importer.
 *
 * Reads a text file formatted as "rowKey:integerValue",
 * and emits the integer value in the specified row at column:
 *     "imported_family:int_value_column"
 *
 * Each line from the input text file is converted into an input key/value pair
 * by the Hadoop text input format, where:
 *   <li> the key is the offset of the line in the input file,
 *        as a LongWritable named 'filePos';
 *   <li> the value is the line content, as a Text object named 'value'.
 */
public class BulkImporterExample extends KijiBulkImporter<LongWritable, Text> {
  public static enum Counters {
    INVALID_INPUT_LINE,
    INVALID_INTEGER,
  }

  /** {@inheritDoc} */
  @Override
  public void produce(LongWritable filePos, Text value, KijiTableContext context)
      throws IOException {
    // Process one line from the input file (filePos is not useful in this example):
    final String line = value.toString();

    // Line is expected to be formatted as "rowKey:integerValue":
    final String[] split = line.split(":");
    if (split.length != 2) {
      // Record the invalid line and move on:
      context.incrementCounter(Counters.INVALID_INPUT_LINE);
      return;
    }
    final String rowKey = split[0];
    try {
      final int integerValue = Integer.parseInt(split[1]);

      // Write a cell in row named 'rowKey', at column 'imported:int_value':
      final EntityId eid = context.getEntityId(rowKey);
      context.put(eid, "imported_family", "int_value_column", integerValue);

    } catch (NumberFormatException nfe) {
      // Record the invalid integer and move on:
      context.incrementCounter(Counters.INVALID_INTEGER);
      return;
    }
  }
}
{% endhighlight %}

This bulk-importer may be run from the console with a command around the lines of:

{% highlight bash %}
kiji bulk-import \
    --importer=pkg.BulkImporterExample \
    --input="format=text file=hdfs://cluster/path/to/text-input-file \
    --output="format=kiji table=kiji://hbase/instance/table nsplits=1" \
{% endhighlight %}

See ??? for a more comprehensive list of options on the command-line interface.


### Provided Library Classes

Within the Kiji MapReduce library, there’s a variety of useful parsers for building your own bulk importer:
* _CSVParser_ - parses delimited CSV(Comma Separated Value) data into the component fields.  This parser also handles TSV(Tab Separated Value) data.
* _CommonLogParser_ - parses Common Log Format data(used by Apache web server) into the relevant fields for each request in the log.

There are several associated bulk importers that parse data into rows:
* _CSVBulkImporter_ - takes in CSV files and produces a row for each line in the file.
* _CommonLogBulkImporter_ - takes in an Apache web server log and produces a row for each client request.
* _JSONBulkImporter_ - takes an a text file with a JSON object on each line and produces a row for each object.

All of these bulk importers extend DescribedInputTextBulkImporter which contains helper functions and can be configured via a KijiTableImportDescriptor object which translates from the inferred schemas inside of the input files to the existing Kiji table layout. These translations also include configuring the EntityId that is used for defining the row key that is uses for the generated rows.

See the javadoc for these classes for instructions on using them.

## Gatherers

### Motivation

A Kiji Gatherer scans over the rows of a Kiji table using the MapReduce framework to extract or aggregate information which can optionally be passed to a Reducer.

### Classes Overview

All gatherers extend `KijiGatherer` which is a MapReduce mapper.

A Gatherer must implement: `getOutputKeyClass()` and `getOutputValueClass()` to specify the classes of output keys and values emitted by the gatherer;
It must also implement `getDataRequest()` to specify the columns it requires while scanning the input table;
Finally, it must implement the logic to process each input row in `gather(KijiRowData input, GathererContext<K, V> context)`,
using the gatherer `context` to emit (key, value) pairs.

Optionally, a gatherer may implement `setup()` and `cleanup()` to initialize and finalize resources that may be shared across the entire gather task.
These methods will be called once by each task, `setup()` before processing input records (i.e. Kiji table rows) and `cleanup()` after the task is done processing input.

### Using the API

### Example

{% highlight java %}
/**
 * Example of a gatherer class.
 *
 * Processes rows from an input table with:
 *   <li> a column 'info:first_name' with users' first names,
 *   <li> a column 'info:last_name' with users' last names,
 *   <li> a column 'info:zip_code' with user's zip codes;
 * and emits key/value pairs to a sequence file on HDFS, where:
 *   <li> keys are zip codes as LongWritable,
 *   <li> values are user full names as Text objects.
 */
public class GathererExample extends KijiGatherer<LongWritable, Text> {
  public static enum Counters {
    MISSING_INPUT,
  }

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputKeyClass() {
    return LongWritable.class;
  }

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputValueClass() {
    return Text.class;
  }

  /** {@inheritDoc} */
  @Override
  public KijiDataRequest getDataRequest() {
    // Fetch all columns in family 'info':
    return KijiDataRequest.create("info");
  }

  /** {@inheritDoc} */
  @Override
  public void gather(KijiRowData input, GathererContext<LongWritable, NullWritable> context)
      throws IOException {
    // Extract the required data from the input row:
    final CharSequence firstName = input.getMostRecentValue("info", "first_name");
    final CharSequence lastName = input.getMostRecentValue("info", "last_name");
    final Integer zipCode = input.getMostRecentValue("info", "zip_code");
    if ((firstName == null) || (lastName == null) || (zipCode == null)) {
      // Some data is missing from the input row, report the bad input and move on:
      context.incrementCounter(Counters.MISSING_INPUT);
      return;
    }

    // Some computation, for this example we concatenate first and last name:
    final String fullName = String.format("%s %s", firstName, lastName);

    // Emit a pair (zip code, full name) to the configured output through the context:
    context.write(zipCode, new Text(fullName));
  }
}
{% endhighlight %}

The gatherer described in the previous section may be used on the command-line as follows:

{% highlight bash %}
kiji gather \
    --gatherer=pkg.GathererExample \
    --input="format=kiji table=kiji://.env/default/table_name" \
    --output="format=seq file=hdfs://cluster/path/to/sequence-file nsplits=1"
{% endhighlight %}

### Provided Library Classes (Optional)


## Producers

### Motivation

A KijiProducer executes a function over a subset of the columns in a table row and produces output to be injected back into a column of that row.
Producers can be run in the context of a MapReduce over entire Kiji tables, or on-demand over a single row at a time.
Common tasks for producers include parsing, profiling, recommending, predicting, and classifying.
For example, you might run a LocationIPProducer to compute and store the location of each user into a new column,
or a PersonalizationProfileProducer to compute a personalization profile.

### Classes Overview

Each producer must extend `KijiProducer` and must implement:
`getDataRequest()` to specify the columns required while scanning from the input table;
`getOutputColumn()` to specify the column or the map-type family being produced;
and finally, the logic to produce the content for the output column for each input row in
`void produce(KijiRowData row, ProducerContext context)`, using the producer `context`
to emit the produced value.

Optionally, a produce may implement `setup()` and `cleanup()` to initialize and finalize resources that can be reused during the produce task.
These methods will be called once by each task, `setup()` before processing input row and `cleanup()` after the task is done processing.

### Using the API

### Example

Following is a minimal example of a Kiji producer class:

{% highlight java %}
/**
 * Example of a producer class.
 */
public static class ProducerExample extends KijiProducer {
  public static enum Counters {
    MISSING_BIRTHDAY,
  }

  /** {@inheritDoc} */
  @Override
  public KijiDataRequest getDataRequest() {
    // Fetch all columns in family 'info' from the input table:
    return KijiDataRequest.create("info");
  }

  /** {@inheritDoc} */
  @Override
  public String getOutputColumn() {
    // Configure the producer to emit to a single column 'produced:zodiac_sign:
    return "produced:zodiac_sign";
  }

  /** Compute the zodiac sign from a birthday. */
  private String zodiacSignFromBirthday(String birthday) {
    // Implementation left to the reader
    // …
  }

  /** {@inheritDoc} */
  @Override
  public void produce(KijiRowData row, ProducerContext context) throws IOException {
    // Extract the required data from the input row:
    final CharSequence birthday = input.getMostRecentValue("info", "birthday");
    if (birthday == null) {
      // Input row contains no birthday, report and move on:
      context.incrementCounter(Counters.MISSING_BIRTHDAY);
      return;
    }

    // Some computation:
    final String zodiacSign = zodiacSignFromBirthday(birthday.toString());

    // Emits the generated content to the configured output column:
    context.put(zodiacSign);
  }
}
{% endhighlight %}

The producer described in the previous section may be used on the command-line as follows:

{% highlight bash %}
kiji produce \
    --producer=pkg.ProducerExample \
    --input="format=kiji table=kiji://.env/default/table_name" \
    --output="format=kiji table=kiji://.env/default/table_name nsplits=1"
{% endhighlight %}

Note: the output table of a producer must match the input table.

### Provided Library Classes (Optional)


## Reducers

### Motivation

Reducers are useful to group, combine or join data sets.

### Classes Overview

A Kiji reducer extends the base class `KijiReducer` and must implement:
`getOutputKeyClass()` and `getOutputValueClass()` to specify the classes of the output keys and values it emits;
and the logic to reduce the set of input values for one input key in `reduce(InputKey key, Iterable<InputValue> values, Context context)`,
using the reducer `context` to emit the reduced key/value pairs to the configured output.

Optionally, a reducer may use `setup()` and `cleanup()` to initialize and finalize resources that can be reused during the reduce task.
These methods will be called once by each task, `setup()` before processing input records and `cleanup()` after the task is done processing input.

### Using the API

### Example

Here is a simplistic example of a reducer class:

{% highlight java %}
/**
 * Example of a reducer class.
 *
 * Sums all the integer input values for each Text input key,
 * and emit the resulting (key, sum) pair.
 */
public class ReducerExample extends KijiReducer<Text, IntWritable, Text, IntWritable>
    implements Configurable {

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputKeyClass() {
    return Text.class;
  }

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputValueClass() {
    return IntWritable.class;
  }

  /** {@inheritDoc} */
  @Override
  protected void reduce(Text key, Iterable<IntWritable> values, Context context)
      throws IOException, InterruptedException {
    // Sums all the integer values for this particular key:
    int sum = 0;
    for (IntWritable value : values) {
      sum += value.get();
    }

    // Emit the sum associated to the original key:
    context.write(key, new IntWritable(sum));
  }

}
{% endhighlight %}

This reducer may be chained to gatherer emitting (Text, IntWritable) pairs as follows:

{% highlight bash %}
kiji gather \
    --gatherer=pkg.GathererExample \
    --reducer=pkg.TableReducerExample \
    --input="format=kiji table=kiji://.env/default/table_name" \
    --output="format=seq file=hdfs://localhost:9000/reducer_output.seq nsplits=1"
{% endhighlight %}

### Provided Library Classes (Optional)

`TODO` Are there reducers worth mentioning here in Kiji MapReduce Library?

### Table reducers

`KijiTableReducer` is a specialization of a `KijiReducer` that writes cell into an output Kiji table.
A table reduce must implement the logic to reduce the input values for an input key in
`reduce(InputKey key, Iterable<InputValue> values, KijiTableContext context)`,
using the table reducer `context` to write cells to the configured output table.

Below is an minimal example of a table reducer:

{% highlight java %}
/**
 * Example of a table reduces, ie. of a reducer that writes back to a Kiji table.
 *
 * Sums the integer input values for each Text input key,
 * and writes the resulting sum to a Kiji table in the row whose entity ID is the key
 * in the column 'reduced:sum'
 */
public static class TableReducerExample extends KijiTableReducer<LongWritable, Text> {
  /** {@inheritDoc} */
  @Override
  protected void reduce(Text key, Iterable<IntWritable> values, KijiTableContext context)
      throws IOException {
    int sum = 0;
    for (IntWritable integer : values) {
      sum += integer.get();
    }
    context.put(context.getEntityId(key.toString()), "reduced", "sum", sum);
  }
}
{% endhighlight %}

This reducer may be used in conjunction with a gatherer as follows:

{% highlight bash %}
kiji gather \
    --gatherer=pkg.GathererExample \
    --reducer=pkg.TableReducerExample \
    --input="format=kiji table=kiji://.env/default/table_name" \
    --output="format=kiji table=kiji://.env/default/table_name nsplits=1"
{% endhighlight %}

## Transforms

### Motivation

A  Kiji Transformer scans over a file in HDFS and uses the MapReduce framework to aggregate information which can be passed to a Reducer.
This is useful for analyzing data that already lives inside of HDFS, but does not live in Kiji.
This allows you to use the same Mappers and Reducers that are used in other Kiji jobs.

### Classes Overview

Transform jobs are created using the KijiTransformJobBuilder.
Transform jobs require more setup because their data does not exist in Kiji, and so the inputs and outputs need to be specified explicitly.
Inside of the KijiTransformBuilder, you'll find the withMapper() and the withReducer methods to be able to explicitly specify your map and reduce tasks.
To specify the type of the input required for this, the withInput method is called

*   `withInput(MapReduceJobInput jobInput)` - specifies the type of the input for a KijiTransformJobBuilder. Possible examples include:

    * `WholeTextFileMapReduceJobInput` - reads entire text files as records for a map task.

    * `SequenceFileMapReduceJobInput` - reads in sequence files

    * `TextMapReduceJobInput` - reads in text files

    * `AvroKeyValueMapReduceJobInput` - reads in Avro container files of generic records where each entry has a "key" and a "value" field

    * `AvroKeyMapReduceJobInput` - reads in Avro container files

`TODO` Can users write their own input classes? How?


### Using the API


### Example

### Provided Library Classes (Optional)


# Command Line Tools

The KijiMR framework provides command-line tools to submit and monitor MapReduce jobs.

## Overview of available tools

Kiji MapReduce provides the following command-line tools:
*   `kiji bulk-import`: runs a bulk-importer job that imports data from an external data source into a Kiji table.
*   `kiji produce`: runs a producer job.
*   `kiji gather`: runs a gatherer job that processes the rows from a Kiji table and writes files, optionally using a reducer.
*   `kiji bulk-load`: bulk-loads the HFile output of a job into a KijiTable.
*   `kiji mapreduce`: runs an arbitrary Map/Reduce job using Kiji mappers and reducers.
*   `kiji job-history`: retrieve information about jobs previously run through kiji if enabled.

## Using common-flags

Kiji commands bulk-import, produce and gather all recognize the following flags:

*   `--kvstores=/path/to/store-specifications.xml`: specifies the path of XML file describing the key/value stores used by the job.

*   `--lib=/path/to/jars-directory/`: specifies a directory of jar files that contain user code.


*   `--start-row=`: specifies the HBase row to start scanning at (inclusive).
    Example: `--start-row=hex:0088deadbeef`, or `--start-row="utf8:the row key in UTF8"`.

*   `--limit-row=`: specifies the HBase row to stop scanning at (exclusive).
    Example: `--limit-row=hex:0088deadbeef`, or `--limit-row=:utf8:the row key in UTF8"`.


Bulk importers must specify the name of the class providing the bulk-import logic:

*   `--importer=java.package.BulkImporterClassName`: specifies the KijiBulkImporter class to use.

Producers must specify the name of the class providing the producers logic:

*   `--producer=java.package.ProducerClassName`: specifies the KijiProducer class to use.

Gatherer must specify the name of the class providing the gathering logic, and optionally a reducing logic:

*   `--gatherer=java.package.GathererClassName`: specifies the KijiGatherer class to use.

*   `--combiner=java.package.CombinerClassName`: optionally specifies a Combiner class to use.

*   `--reducer=java.package.ReducerClassName`: optionally specifies a Reducer class to use.

## Input/output formats

Jobs inputs and outputs are specified with the following flags:

*   `--input=...`: specifies the input of the job.

    The job input specification is formatted as `--input="format= ..."`.
    Kiji recognizes the following job input formats:

    * `avro`: job input is an Avro container file, each input record is a pair (Avro key, NullWritable).

    * `avrokv`: job input is an Avro container file for key/value generic records.

    * `htable`: job input is an HTable, each input record is a pair (HBase row key, HBase Result).
      The address of the HBase cluster is pull from the local job configuration (ie. from the HBase configuration available on the classpath).
      Example: `--input="format=htable htable=htable-table-name"`.

    * `kiji`: job input is a Kiji table, each input record is a pair (row entity ID, KijiRowData).
      Example: `--input="format=kiji table=kiji://.env/default/input_table"`.

    * `seq`: job input is a Hadoop sequence file.
      Example: `--input="format=seq file=hdfs://dfsmaster:9000/path/to/sequence-file/"`.

    * `small-text-files`: job input is a set of small text files, each input record is a pair (text file path, text file content).
      Example: `--input="format=small-text-files file=hdfs://dfsmaster:9000/path/to/text-files/"`.

    * `text`: job input is a text file, each input record is a pair (position in the text file, line of text).
      Example: `--input="format=text file=hdfs://dfsmaster:9000/path/to/text-file/"`.

*   `--output=...`: specifies the output of the job.

    The job specification is formatted as: `--output="format= nsplits=N ..."`.
    Kiji recognizes the following job output formats:

    * `avro`: job output is an Avro container file, each output record is a pair (Avro key, NullWritable).

    * `avrokv`: job output is an Avro container file with key/value generic records.

    * `hfile`: job output is an HFile that will be bulk-loaded into a Kiji table.
      Example: `--output="format=hfile nsplits=10 table=kiji://.env/default/target_table file=hdfs://dfsmaster:9000/path/to/hfile/"`.

    * `kiji`: job output is a Kiji table.
      The use of this job output should be limited to development only and should not be used in production as it may incur high load on the target HBase cluster.
      Example: `--output="format=kiji nsplits=10 table=kiji://.env/default/target_table"`.

    * `map`: job output is a Hadoop map file.
      Example: `--output="format=map nsplits=10 file=hdfs://dfsmaster:9000/path/to/map-file/"`.

    * `seq`: job output is a Hadoop sequence file.
      Example: `--output="format=seq nsplits=10 file=hdfs://dfsmaster:9000/path/to/sequence-file/"`.

    * `text`: job output is a text file; each (key, value) record is written to a text file with a separator
      (via the configuration parameter "mapred.textoutputformat.separator", which defaults to TAB) and a new line.
      Example: `--output="format=text nsplits=10 file=hdfs://dfsmaster:9000/path/to/text-file/"`.

# Testing

### Motivation

Kiji comes with a testing framework that makes it very easy to ensure that code you have written works as expected.

### Setup

To use the Kiji testing framework, you must depend on the Kiji testing artifact: `org.kiji.schema:kiji-schema:<kiji-version>:test-jar`.

{% highlight xml %}
<dependency>
  <groupId>org.kiji.schema</groupId>
  <artifactId>kiji-schema</artifactId>
  <version>${kiji-schema.version}</version>
  <type>test-jar</type>
  <scope>test</scope>
</dependency>
{% endhighlight %}

### Classes Overview

Kiji unit tests inherit from the base class `KijiClientTest` which provides an environment suitable for tests of Kiji schema logic and MapReduce logic.
It provides a testing Hadoop configuration accessible through `KijiClientTest.getConf()`.
The KijiClientTest base class keeps track of Kiji instances created for testing.
In particular, it provides a default Kiji instance accessible with `KijiClientTest.getKiji()`.
Other Kiji instances may be created with `KijiClientTest.createKiji()`.

The Kiji testing framework also provides an `InstanceBuilder` helper class to define and populate testing Kiji environments.

### Using the API


{% highlight java %}
public class SomeTests extends KijiClientTest {
  @Test
  public void testSomeFeature() throws Exception {
    // Use the default testing Kiji instance managed by KijiClientTest.
    // No need to release this instance, it is take care of by KijiClientTest.
    final Kiji kiji = getKiji();

    // Create or load a table layout:
    final KijiTableLayout tableLayout = ...;
    final String tableName = tableLayout.getName();

    // Populate the existing Kiji instance 'kiji':
    new InstanceBuilder(kiji)
        // Declare a table
        .withTable(tableName, tableLayout)
            // Declare a row for the entity "Marsellus Wallace":
            .withRow("Marsellus Wallace")
                 .withFamily("info")
                     .withQualifier("first_name").withValue("Marsellus")
                     .withQualifier("last_name").withValue("Wallace")
            // Declare another row for the entity "Vincent Vega":
            .withRow("Vincent Vega")
                 .withFamily("info")
                     .withQualifier("first_name").withValue("Vincent")
                     .withQualifier("last_name").withValue("Vega")
        .build();

    // Open the test table:
    final KijiTable table = kiji.openTable(tableName);
    try {
      // Use the populated table:
      // …
    } finally {
      table.close();
    }
  }
}
{% endhighlight %}

### Example (Unit and integration/single MapReduce job and mutliple in a row)


### Provided Library Classes (Optional)



# KeyValue Stores

### Motivation

KeyValueStores are used to provide MapReduce programs and other operators processing Kiji datasets with the ability to join datasets. One data set can be specified as a key-value store using the KeyValueStore API. The program can use a KeyValueStoreReader to look up values associated with keys. These keys are often driven by records of a dataset being processed by MapReduce.

This can be used, for example, to provide a `KijiProducer` with the means to apply the results of a trained machine learning model to the main data set. The output of a machine learning model might be expressed as (key, value) pairs stored in files in HDFS, or in a secondary Kiji table. For each user in a users table, you may want to compute a new recommendation for the user by applying the model to the information in the user's row. A value in the user's row may be a key into some arbitrary key-value store representing the model; the returned value is the recommendation.

You may also need to perform "ordinary" map-side joins in a MapReduce program, e.g., for denormalization of data. The smaller dataset can be held in RAM in each map task in the form of a KeyValueStore. For each record in the larger dataset, you can look up the corresponding small-side record, and emit the concatenation of the two to the reducer.

`KeyValueStores` also allow non-MapReduce applications to read key-value pairs from HDFS-backed datasets in a file-format-agnostic fashion. For example, you may run a MapReduce program that emits output as a `SequenceFile` that your frontend application needs to consume. You could use a `SequenceFile.Reader` directly, but if you ever change your MapReduce pipeline to emit to text files or Avro files, you will need to rewrite your client-side logic. Using the KeyValueStoreReader API in your client allows you to decouple the act of using a key-value map from what format you need to use to read the data.

### Classes Overview

The main classes in the KeyValueStore API are `org.kiji.mapreduce.kvstore.KeyValueStore` and `org.kiji.mapreduce.kvstore.KeyValueStoreReader`.

A `KeyValueStore` specifies all the resources needed to surface key-value pairs from some backing store. This may be defined on files, a Kiji table, or some other resource like a different NoSQL database.

Several KeyValueStore implementations are made available in the `org.kiji.mapreduce.kvstore.lib` package that cover common use cases for file- or Kiji-backed datasets. You could write your own `KeyValueStore` implementation that accesses a foreign system (e.g., a Redis database).

A `KeyValueStore` class specifies how to read data into the store. For example, `TextFileKeyValueStore` expects to parse delimited text files, while `SeqFileKeyValueStore` reads SequenceFiles. Both of these stores are configured with an HDFS path to read from. A `KijiTableKeyValueStore` requires different configuration.

The KeyValueStore implementations provided in the library are immutable; they’re all created through builder classes. They each have a method named `builder()` that returns a new instance of the associated builder class.

The `KeyValueStoreReader` API is used to actually look up values by key, from some KeyValueStore. You cannot directly instantiate any concrete implementations of `KeyValueStoreReader` yourself; use a given KeyValueStore's `open()` method to open an associated reader object. The client is agnostic to the backing store; one `KeyValueStoreReader` should act the same as the next, given equivalent backing data.

By default, a KeyValueStoreReader's data is presented to you as a read-only, non-iterable map. Only `get()` requests for an explicit key are supported by default, though some implementations may offer iteration.

An opened KeyValueStoreReader may contain state or connect to external resources; you should call the `close()` method when you are finished using it.

#### Simple Example for Opening a KeyValueStore

{% highlight java %}
    final KeyValueStore<String, String> csvKeyValues = TextFileKeyValueStore.builder()
        .withInputPath(new Path("some-file.txt"))
        .withDistributedCache(false)
        .build();

    final KeyValueStoreReader<String, String> reader = csvKeyValues.open();
    try {
      String theValue = reader.get("some-key");
      System.out.println("Contained a mapping: some-key -> " + theValue);
    } finally {
      reader.close();
    }
{% endhighlight %}

### Using the API

The distinction between KeyValueStore and KeyValueStoreReader is intentional. A given MapReduce program may need to access some set of KeyValueStoreReaders. When the job is configured, you must configure it with any associated KeyValueStores. For example, file-backed stores often use the DistributedCache to efficiently copy files to all map tasks. This must be configured before the job begins.

Your MapReduce task class (KijiMapper, KijiReducer, KijiProducer, KijiGatherer, etc) can implement `KeyValueStoreClient` to help specify to a Kiji job builder (e.g., KijiGatherJobBuilder) that it requires stores. This will require that you implement a method named `getRequiredStores()` that returns a mapping from names to KeyValueStores. These are the default _bindings_ for KeyValueStores.

The use of Java generic types makes constructing the return value from your `getRequiredStores()` method to be cumbersome. Some static factory methods have been added for your convenience in the `RequiredStores` class. For example, to require no stores:

{% highlight java %}
    @Override Map<String, KeyValueStore<?, ?>> getRequiredStores() {
      return RequiredStores.none();
    }
{% endhighlight %}

To require one empty store named `"mystore"`:

{% highlight java %}
    @Override Map<String, KeyValueStore<?, ?>> getRequiredStores() {
      RequiredStores.just("mystore", EmptyKeyValueStore.get());
    }
{% endhighlight %}

The `RequiredStores.with()` method will return a `Map` object augmented with a `with(String, KeyValueStore)` method, so you can call `RequiredStores.with("foo", store1).with("bar", store2);`.

Within the `KijiGatherer.gather()` method, the `KijiContext` object provided as an argument will provide you with access to KeyValueStoreReaders. Since you may need multiple KeyValueStores, you can refer to each by the name you bound it to in `getRequiredStores()`.

Since each call to `gather()` is likely to require the same stores, it would be high-overhead to open and close a KeyValueStoreReader in every call. The KijiContext itself uses a class called `KeyValueStoreReaderFactory`, which is responsible for instantiating KeyValueStores from the `Configuration` associated with the job, and maintaining a pool of lazily-opened KeyValueStoreReader instances organized by name binding. The gatherer itself does not close individual KeyValueStoreReader instances. The KeyValueStoreReaderFactory will close all of them as the task is being torn down.

#### Overriding Default Bindings in Job Builders

While the `getRequiredStores()` method allows you to define bindings between names and implementations, specific invocations of the MapReduce job may require that you override these implementations. For example, you may want to use one HDFS path as input in production, but a different HDFS path in local tests.

The MapReduceJobBuilder subclasses all support a method called `withStore(String name, KeyValueStore store)`. This method allows you to specify a different KeyValueStore implementation for that particular job. You will need to override a store name that the job will expect to read; i.e., if `getRequiredStores()` returned a binding from `"mystore"` to a particular TextFileKeyValueStore, you should call `myJobBuilder.withStore("mystore", myDifferentKeyValueStore);`.

#### Overriding Default Bindings on the Command Line

You can also override KeyValueStore bindings on the command line with the `--kvstores` argument. This argument specifies an XML file that should look like the following:

{% highlight xml %}
<stores>
  <store name="mystore" class="org.kiji.mapreduce.kvstore.lib.TextFileKeyValueStore">
    <configuration>
      <property>
        <name>paths</name>
        <value>/path/to/foo.txt,/path/to/bar.txt</value>
      </property>
      <property>
        <name>delim</name>
        <value>,</value>
      </property>
    </configuration>
  </store>
</stores>
{% endhighlight %}


This example defines a delimited-text-file store, bound to the name `"mystore"`. It reads from `foo.txt` and `bar.txt` and expects a comma between the key and value fields on each line.

You can define multiple name-to-store bindings with different `<store>` blocks with unique names. The properties available within each store's configuration is specified in the Javadoc for each KeyValueStore class.

#### Requiring Runtime Configuration of KeyValueStores

If you know that your KijiProducer requires a store named `"mystore"`, but do not know at compile time where the physical resource that backs it will be, you may want to force your clients to specify this at runtime using one of the above two methods. A mapping of `RequiredStores.just("mystore", UnconfiguredKeyValueStore.get());` will throw an exception when it is serialized to the `Configuration`, which ensures that your job cannot start unless you override the definition with an XML file or a binding in a job builder.


### Example

In this example, suppose we have computed a set of movie sequels, which we will use to naively recommend to users on our web site that if they've watched the first one, they watch the sequel. Suppose we stored the movie sequel dataset in pipe-delimited text files in HDFS, that look something like:

    Star Wars: A New Hope|Star Wars: The Empire Strikes Back
    Raiders of the Lost Ark|Indiana Jones and the Temple of Doom
    ...

We could write a KijiProducer that read from the `info:recently_watched` column and produce a field named `info:recommended_movie` as follows:

{% highlight java %}
    import java.io.IOException;

    import org.apache.hadoop.fs.Path;

    import org.kiji.mapreduce.*;
    import org.kiji.mapreduce.kvstore.*;
    import org.kiji.mapreduce.kvstore.lib.*;
    import org.kiji.schema.*;

    /**
     * Produce a movie recommendation based on the theory that if they liked
     * the first one, they should watch the sequel.
     */
    public class MoveRecProducer extends KijiProducer {
      @Override public Map<String, KeyValueStore<?, ?>> getRequiredStores() {
        // Note that it's ok to specify a path to an HDFS dir, not just one file.
        return RequiredStores.just("sequels", TextFileKeyValueStore.builder()
            .withDelimiter("|")
            .withInputPath(new Path("/path/to/movie-sequels.txt")).build());
      }

      @Override KijiDataRequest getDataRequest() {
        return KijiDataRequest.create("info", "recently_watched");
      }

      @Override String getOutputColumn() {
        return "info:recommended_movie";
      }

    @Override void produce(KijiRowData input, ProducerContext context)
        throws IOException {

        if (!input.containsColumn("info", "recently_watched")) {
          // No basis for recommendation.
          return;
        }

        // Note that we don't call our own getRequiredStores() method. That was used
        // in the configuration phase of the job, not within each task. Its output may have
        // been overridden at run-time by the user. We take the deserialized store from the
        // ProducerContext instead.
        KeyValueStoreReader<String, String> sequelStore = context.getStore("sequels");

        String lastWatched = input.getMostRecentValue("info", "recently_watched").toString();
        String nextMovie = sequelStore.get(lastWatched);
        if (null != nextMovie) {
          // We found a match! Write to info:recommended_movie
          context.write(nextMovie);
        }
      }
    }
{% endhighlight %}


### Provided Library Classes

Several implementations of KeyValueStore are available in the `org.kiji.mapreduce.kvstore.lib` package:

Several file-backed KeyValueStore implementations provide access to stores in different file formats:

* `AvroKVRecordKeyValueStore` - Key-Value pairs specified in Avro records containing two fields, `key` and `value`
* `AvroRecordKeyValueStore` - Avro records in an Avro file, to be indexed by a configurable field of each record.
* `SeqFileKeyValueStore` - Key-Value pairs in SequenceFiles
* `TextFileKeyValueStore` - string key-value pairs in delimited text files

You can also access a specific column of a Kiji table by the row's entityId using the `KijiTableKeyValueStore`.

If you want to declare a name binding on a KeyValueStore whose exact configuration cannot be determined before runtime, use the `UnconfiguredKeyValueStore`. It will throw an IOException in its `storeToConf()` method, ensuring that your `MapReduceJobBuilder` must call `withStore()` to override the definition before launching the job.

The `EmptyKeyValueStore` is a good default choice when you plan to override the configuration at runtime, but find it acceptable to operate without this information.
